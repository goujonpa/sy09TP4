\documentclass{report}
\usepackage[]{algorithm2e}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[frenchb]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{listingsutf8}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}

\lstloadlanguages{R}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
\lstset{
    language=R,
    basicstyle=\footnotesize,
    numbers=left,
    backgroundcolor=\color{white},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{green},
    extendedchars=true,
    keepspaces=true,
    keywordstyle=\bfseries\color{blue},
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    showtabs=false,
    stringstyle=\color{red},
    tabsize=2,
    title=\lstname
}

\title{SY09 - TP4 Discrimination}
\date{Juin 2016}
\author{Stéphane LOUIS et Paul GOUJON}
\pagenumbering{gobble}
\maketitle

\newpage
\tableofcontents{}

\newpage
\pagenumbering{arabic}
\chapter{Introduction}

\paragraph{Contenu}
Ce document présente les résultats obtenus par notre binôme au cours de ce quatrième TP de SY09.

\paragraph{Annexes}
Vous trouverez en annexes de ce TP tous les scripts nous ayant servi lors de l'élaboration de ce TP, ainsi qu'un dossier contenant nos résultats expérimentaux. Vous trouverez également quelques précisions concernant la fonction de chacun des scripts au sein de la partie intitulée "Programmation"

\paragraph{Objectifs}
Les objectifs de ce TP sont multiple. Le principal d'entre eux est l'étude de différents modèles de classifieurs, sur différents jeux de données, et en comparer les performances. Ces interprétations devront être liées aux hypothèses assumées vérifiées par chacun des classifieurs, ainsi que le nombre de paramètres estimés au cours de leur mise en oeuvre. Afin d'atteindre cet objectif, nous devrons programmer les différents classifieurs en question, afin de jongler amplement entre concepts mathématiques et capacités techniques de programmation en \verb+R+.

\chapter{Etude}

\section{Démarche}
1. Examen visuel


\newpage
\subsection{BCW}
\paragraph{Résultats}
Le tableau ci-dessous récapitule les taux d'erreurs et intervalles de confiance de nos différents classifieurs sur le jeu de données \verb+BCW+ :

\begin{table}[h!]
    \centering
    \caption{Estimations des taux d'erreurs et intervalles de confiances, par classifieur, sur le jeu de données "BCW"}
    \label{tab:table1}
    \def\arraystretch{1.5}
    \begin{tabular}{c||c|c|c}
        \hline
        & ADQ & ADL & NBA\\
        \hline
        $\varepsilon$ & 0.052 & 0.045 & 0.039\\
        \hline
        $IC$ & $[0.024 ; 0.082]$ & $[0.018; 0.072]$ & $[0.014 ; 0.064]$\\
        \hline
        \hline
        & LOGBT & LOGBF & TREE\\
        \hline
        $\varepsilon$ & 0.039 & 0.158
        & 0.059\\
        \hline
        $IC$ & $[0.014 ; 0.064]$ & $[0.110 ; 0.205]$
        & $[0.028 ; 0.089]$\\
        \hline
        \hline
    \end{tabular}
\end{table}

\paragraph{Interprétation}
Ce dernier dataset produit de meilleurs résultats de la part de nos classifieurs. Peut être sommes nous de retour dans un cas se rapprochant plus du fameux "cas Gaussien" ? Cela expliquerait les bien meilleures performances de nos classifieurs. Nous remarquons directement que les classifieurs fournissant les meilleurs résultats sont ceux pour lesquels nous estimons le moins grand nombre de paramètre (NBA, Regression Logistique classique...) : nous pouvons émettre l'hypothèse que leurs meilleures performances est liée à leur meilleure robustesse présumée. Notons également les mauvaises performances de la Regression Logistique classique sans ajout d'ordonnée à l'origine, comparée aux autres classifieurs, qui semble une nouvelle fois confirmer la nécessité de cet ajout.

\newpage
\section{Conclusion}
\paragraph{Interprétations \& Conclusions}
De ces nombreux tests, nous tirons un certain nombre d'enseignements :
\begin{itemize}
    \item \textbf{Regression Logistique et ajout d'ordonnée à l'origine} : Il semble que l'ajout de cette fameuse ordonnée à l'origine soit un choix intéressant, pour donner plus de liberté à notre frontière de décision, et lui permettre ainsi de mieux épouser les différentes classes à discriminer.
    \item \textbf{Hypothèses et choix des classifieurs} : les performances d'un classifieur dépendra toujours des hypothèses que ce dernier assume comme vérifiées, et donc de la vérification de ces hypothèses au sein du jeu de données étudié. Il est donc nécessaire de trouver, pour chaque type de jeu de données, le classifieur le plus adapté, en émettant les bonnes hypothèses.
    \item \textbf{Robustesse contre spécificité} : Il est également nécessaire de trouver un certain juste milieu entre robustesse et spécificité, en prenant en compte le fait que généralement, plus on estime un grand nombre de paramètres lors de l'entrainement d'un classifieur, plus on perd en robustesse ce que l'on gagne en spécificité. Il est donc nécessaire de trouver le juste milieux, matchant au mieux nos besoins.
    \item \textbf{Evaluation des performances d'un classifieur} : L'estimation du risque des classifieurs envisagés, notamment grâce à des méthodes telles que la "méthode de l'ensemble de validation", la "validation croisée", ou encore le "bootstrap", est un bon indicateur des performances de chacun de ces derniers sur le jeu de données considéré.
\end{itemize}

\chapter{Conclusion}
\paragraph{Conclusion}
Ce TP a une nouvelle fois été l'occasion pour nous d'appliquer les connaissances théoriques concernants les différents classifieurs abordés en cours. Nous avons eu l'occasion d'implémenter et de tester l'Analyse Discriminante Quadratique, l'Analyse Discriminante Linéaire, le Classifieur Bayesien Naïf, la Regression Logistique classique, la Regression Logistique Quadratique, et pour finir les Arbres de décision. Nous permettant non seulement de consolider nos connaissances théoriques, ce TP a également été l'occasion de continuer nos progrès d'un point de vue technique, via les nombreuses heures passées devant notre écran à tenter de débuguer certains scripts légèrement récalcitrants. Finalement, ce TP nous a ouvert les yeux sur la nécessité de toujours se montrer précautionneux lorsqu'il s'agissait de choisir un modèle de classifieur (notamment pour les raisons abordées au sein de la conclusion de la partie précédente), ainsi que la nécessité d'en évaluer les performances avant de conclure sur sa pertinence.


\end{document}
