\documentclass{report}
\usepackage[]{algorithm2e}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[frenchb]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{listingsutf8}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}

\lstloadlanguages{R}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
\lstset{
    language=R,
    basicstyle=\footnotesize,
    numbers=left,
    backgroundcolor=\color{white},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{green},
    extendedchars=true,
    keepspaces=true,
    keywordstyle=\bfseries\color{blue},
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    showtabs=false,
    stringstyle=\color{red},
    tabsize=2,
    title=\lstname
}

\title{SY09 - TP4 Discrimination}
\date{Juin 2016}
\author{Stéphane LOUIS et Paul GOUJON}
\pagenumbering{gobble}
\maketitle

\newpage
\tableofcontents{}

\newpage
\pagenumbering{arabic}
\chapter{Introduction}

\section{Avant propos}
\paragraph{Contenu}
Ce document présente les résultats obtenus par notre binôme au cours de ce quatrième TP de SY09.

\section{Code \& Résultats : Annexes}
\paragraph{Annexes}
TO DO
\begin{enumerate}
    \item \textbf{ceuc.R} : Les fonctions concernant le classifieur euclidien.
    \item \textbf{kppv.R} : Les fonctions concernant les k plus proches voisins.
    \item \textbf{TP3.R} : Le script que nous avons utilisé au long de la partie évaluation des performances.
    \item \textbf{Dossier "results"} : Les exports en \verb+.csv+ de nos résultats.
\end{enumerate}

\section{Objectifs du TP}
\paragraph{Objectifs}
TODO
Les objectifs de ce TP sont multiple. Le principal d'entre eux est de commencer à nous familiariser avec les concepts d'apprentissage supervisé abordés en cette fin de semestre en SY09. Nous aborderons différents types de classifieur, nous permettant d'effectuer des tâches de discrimination entre individus, comme la classifieur euclidien, la méthode des k plus proches voisins, ou encore la règle de Bayes et appliquerons ces différents concepts notamment en codant les principales fonctions d'apprentissage et de test de ces derniers en \verb+R+.

\chapter{Programmation}
\section{Introduction}
\paragraph{Objectif}
Notre objectif est dans un premier temps de programmer sous \verb+R+ les scripts effectuant l'entrainement et le test de plusieurs modèles de classifieurs étudiés en cours que nous expliciterons dans une deuxième partie de rapport :

\begin{itemize}
    \item L'Analyse Discriminante Quadratique (ADQ)
    \item L'Analyse Discriminante Linéaire (ADL)
    \item Le Classifieur Bayesien Naïf (NBA)
    \item La Regression Logistique binaire
    \item La Regression Logistique quadratique
    \item Les Arbres de décision
\end{itemize}

\paragraph{Code \& Résultats}
Notre code est fourni en annexes. Voici quelques précisions le concernant:
\begin{itemize}
    \item \textbf{adq.R, adl.R, nba.R, ad.R} : contiennent nos implémentations de l'ADQ, l'ADL, le NBA, et \verb+ad.val+, fonction effectuant la prédiction en fonction du modèle d'Analyse Discriminante choisi.
    \item \textbf{logb.R, logq.R, log.R} : contiennent nos implémentation de la Regression Logistique binaire, de la Regression Logistique quadratique, et \verb+log.val+, fonction effectuant la prédiction en fonction des paramètres estimés.
    \item \textbf{prob.ad.R, prob.log.R, prob.log2.R} : nous avons effectué de nombreuses modifications pour adapter ces différentes fonctions à nos besoins (notamment pour être en mesure d'automatiser tout le TP4).
    \item \textbf{TP4.R} : ce script permet d'effectuer tous les tests demandés au sein de ce TP4 de manière automatique, et range les résultats correctement dans notre dossier \verb+results+.
    \item \textbf{tree.R} : nous avons utilisé ce script pour les tests à l'aide des arbres de décisions, étant donné que nous avons eu quelques difficultés à l'intégrer au sein de notre fichier \verb+TP4.R+
    \item \textbf{postPr.R} : nous sert à calculer les probabilités à posteriori au cours de la Regression Logistique.
    \item \textbf{generateQuadX.R} : une fonction que nous avons codé pour générer un jeu de données d'apprentissage quadratique dans le cas de la Regression Logistique quadratique.
    \item \textbf{errorRate.R} : contient deux fonctions que nous avons utilisé pour nos calculs de taux d'erreurs et intervalles de confiance.
\end{itemize}



\chapter{Modèles de classifieurs}
\paragraph{Introduction}
Détaillons au sein de cette section les aspects théoriques de chacun des classifieurs utilisés au sein de ce TP.

\section{Analyse Discriminante}
\paragraph{Introduction}
L'ADQ, ADL et NBA, explicités ci-après sont trois modèles de classifieur découlant de la Théorie Bayesienne de la décision. Ils visent tous à minimiser une fonction de risque, dépendant des probabilités à priori et à posteriori d'appartenance d'un individu à une classe $\omega_k$. Ces modèles assument tout les trois que le vecteur de caractéristique $X$ suit, conditionnellement à chaque classe $\omega_k$, une loi normale multidimensionnelle d'espérance $\mu_k$ et de variance $\Sigma_k$. En faisant différentes hypothèses sur les paramètres de ces lois, on obtient ainsi différentes expressions de a règle de Bayes, d'où l'on déduit différentes règles de décision en remplaçant les paramètres théoriques par leurs estimations.

\subsection{ADQ - Analyse Discriminante Quadratique}
L'ADQ est en quelques sortes le "cas général", dans lequel la distribution de $x$ dans la chaque classe est caractérisée par des paramètres $\mu_k$ et $\Sigma_k$ différents. On a alors :

$$f_k(x) = f(x | \omega_k) = \frac{1}{(2\pi)^{\frac{p}{2}}(det \Sigma_k)^{\frac{1}{2}}}exp(-\frac{1}{2}(x - \mu_k)^T\Sigma_k^{-1}(x - \mu_k))$$


La règle de Bayes s'écrit alors :


$$\delta^{*}(x) = a_{k*}$$

\newpage
avec


$$k^* = \argmax_k \mathbb{P}(\omega_k | x)$$

avec, dans le cas de deux classes,

$$\mathbb{P}(\omega_k | x) = \frac{\pi_k f_k(x)}{\pi_1 f_1(x) + \pi_2 f_2(x)}$$


\paragraph{Implémentation - Note}
Au sein de ce TP, nous avons utilisé la fonction \verb+mvdnorm+ fournie pour le calcul des densités conditionnelles.

\paragraph{Estimation des paramètres}
Dans le cas de l'ADQ, nous avons utilisé les estimateurs sans biais suivants pour les estimations de chacun des paramètres :

$$ \hat{\pi_k} = \frac{n_k}{n}$$

$$ \hat{\mu_k} = \bar{x_k} = \frac{1}{n_k}\sum_{i=1}^n z_{ik}x_i $$

$$ \hat{\Sigma_k} = V_k^* = \frac{n_k}{n_k - 1} V_k$$

avec

$$ V_k = \frac{1}{n_k}\sum_{i=1}^n z_{ik}(x_i - \hat{\mu_k})(x_i - \hat{\mu_k})^T$$

\subsection{ADL - Analyse Discriminante Linéaire}
\paragraph{Introduction}
L'Analyse Discriminante Linéaire est une variante de l'Analyse Discriminante quadratique, en supposant cette fois que la matrice de variance est commune à toutes les classes (cette hypothèse est l'hypothèse d'homoscédasticité).

\paragraph{Estimateurs}
Nos $\hat{\pi_k}$ et $\hat{\mu_k}$ ne changent pas, mais notre matrice de variance devient :


$$ V_W^* = \frac{1}{n-g} \sum_{k=1}^g (n_k-1)V_k^*$$

\subsection{NBA - Classifieur Bayesien Naïf}
\paragraph{Introduction}
Pour finir, nous supposer l'indépendance des variables $X_j$ conditionnellement à $Z$, ce qui, dans le modèle gaussien, revient à supposer les matrices $\Sigma_k$ diagonales. Il est également possible de conjuguer cette hypothèse avec l'hypothèse avec celle d'homoscédacité, on obtient alors une variante de l'ADL dans laquelle la matrice de variance commune $\Sigma$ est estimée par diag($V$), c'est à dire que l'on annule dans $V$ tous les termes non diagonaux. Ce sont les hypothèses que nous avons appliquées pour notre Classifieur Bayesien Naïf au cours de ce TP.

\paragraph{Estimateurs}
De nouveau, nos $\hat{\pi_k}$ et $\hat{\mu_k}$ ne changent pas, mais notre matrice de variance devient :

$$ \Sigma = \text{diag}(V) $$

\section{Regression Logistique}
\paragraph{Introduction}
Nous venons de voir différents modèles se rapportant à l'Analyse Discriminante, se basant sur l'hypothèse que les données suivent dans chaque classe une loi normale. Ces estimations sont d'autant plus précises que les hypothèses portant sur la distribution des données sont vérifiées. Plutôt que de faire des hypothèses sur les distributions conditionnelles $f_k$, l'approche de la Regression Logistique consiste à estimer directement les probabilités d'appartenance aux classes.

\subsection{Modèle général}
\paragraph{Introduction}
L'idée à la base de la régression logistique consiste à modéliser les probabilités à posteriori $\mathbb{P}(\omega_k|x)$ par des fonctions de $x$, choisies de manière à satisfaire naturellement les contraintes $\sum_{k=1}^p \mathbb{P}(\omega_k|x) = 1$ et $\mathbb{P}(\omega_k|x) \epsilon [0;1]$ pour tout $x$.


\paragraph{Estimation de probabilité à posteriori selon le modèle logit}
Nous utilisons dans le cadre de ce TP le modèle dit "logit" pour l'estimation de probabilités à posteriori d'appartenance de $x$ à la classe $k$. Nous obtenons, dans le cas général :

$$\mathbb{P}(\omega_k|x) = \frac{\exp(w_k^T x)}{1 + \sum_{l=1}^{g-1} \exp (w_l^T x)}$$


\newpage
soit, dans le cas de deux classes :


$$\mathbb{P}(\omega_1|x) = \frac{\exp(w^T x)}{1 + \exp (w^T x)}$$


\paragraph{Apprentissage des paramètres}
Nous devons donc, afin de pouvoir déterminer les probabilités à posteriori d'appartenance à une classe d'un individu, estimer le paramètre $w$ nécessaire à notre Regression Logistique. Ce $w$ est un vecteur de poids par lesquels nous allons multiplier les composantes correspondantes de $x$, afin d'en calculer la probabilité.

\paragraph{Log-Vraissemblance}
Afin d'estimer les valeurs des composantes de $w$, nous allons tenter de maximiser la fonction de Log Vraissemblance associée à la loi de Bernouilli prenant les valeurs :

$$T =
    \begin{cases}
        1, & \text{si } Z = \omega_1 \\
        0, & \text{si } Z = \omega_2
    \end{cases}
$$


De laquelle on déduit la fonction de Log-Vraissemblance :

$$ \log L(w; t_1,...,t_n) = \sum_{i=1}^{n} (t_i w^T x_i - \log (1 + \exp (w^T x_i)))$$

Ainsi que le gradient de la Log-Vraissemblance :


$$\frac{\delta \log L (w)}{\delta w} = X^T (t - p) $$


Et l'expression du terme général de la matrice Hessienne :


$$ \frac{\delta^2 \log L(w)}{\delta w_j \delta w_l} = - \sum_{i=1}^{n}x_i^j x_i^l p(x_i;w)(1-p(x_i;w))$$


Soit $W$ la matrice diagonale de terme général $W_ii = p(x_i;w)(1-p(x_i;w))$ on a donc :


$$ H = -X^T W X $$


Ainsi, l'équation de vraissemblance:


$$ \frac{\delta \log L(w)}{\delta w} = 0$$


est un système de $p+1$ équations non linéaires par rapport à $w$.

\paragraph{Algorithme de Newton Raphson}
On ne peut résoudre ce système directement : il faut donc rechercher le vecteur $w$ qui maximise $\log L$ en utilisant un algorithme d'optimisation itératif. Nous utilisons dans ce TP l'algorithme de Newton-Raphson. Cet algorithme consiste à faire, à la $q^e$ itération un développement limité de la fonction à maximiser (ici $\log L(w)$) au voisinage de l'estimation courante $w^{(q)}$ de la solution. La méthode de Newton-Raphson consiste donc à sélectionner un vecteur de poids initial $w{0}$, puis à calculer une séquence de vecteurs $w^1, ...$, qui converge vers un maximum local de la Log-Vraissemblance. En pratique, on arrête de calculer de nouvelles estimations $w^{(q)}$ une fois qu'un certain critère d'arrêt est vérifié (dans notre TP, nous avons arrêté d'itérer dès que la norme de la différence entre deux estimations successives $\beta^{(q)}$ et $\beta{(q+1)}$ est inférieur à un seuil $\varepsilon$ que nous avons fixé à $\varepsilon = 1e -5$). Nous avons utilisé le vecteur nul comme vecteur de poids initial $w^{(0)}$. La mise à jour de notre vecteur de poids se fait donc en appliquant la formule suivante :


$$w^{(k+1)} = w^{(q)} + (X^T W_{(q)}X)^{-1}X^T(t-p^{(q)})$$


\subsection{Regression Logistique quadratique}
\paragraph{Introduction}
La Regression Logistique quadratique est une généralisation assez simple de la Regression Logistique consistant à transformer les données dans un espace plus complexe, dans lequel les classes peuvent être séparées par un hyperplan. La régression logistique est alors effectuée dans cet espace. Ce modèle est plus flexible, mais le nombre de paramètres à estimer étant plus important, il peut également s'avérer moins robuste que le modèle classique déterminé dans l'espace des caractéristiques initiales.

\paragraph{Principe}
Pour le mettre en application, on calcule les produits des variables décrivant les individus d'apprentissage. On apprend ensuite le modèle logistique sur les $\frac{p(p + 3)}{2}$ nouvelles variables ainsi obtenues. L'exemple ci-dessous illustre le produit des variables décrivant les individus d'apprentissage :

$$ X = \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}$$


devient


$$ X_2 = \begin{pmatrix} 1 & 3 & 3 & 1 & 9 \\
2 & 4 & 8 & 4 & 16 \end{pmatrix}$$

Où la troisième colonne est égale au produit des deux dimensions de X, et les deux dernières colonnes sont les dimensions de X, chacune au carré.

\newpage

\section{ORDONNEE A L'ORIGINE ICI}

\subsection{Arbres de décision}
\paragraph{Introduction}
Le principe des arbres de décision, s'il peut être résumé trivialement, est de partitionner récursivement l'espace des individus en sous-régions de décisions les plus pures possible de manière à satisfaire un critère de pureté, dépendant d'une mesure d'impureté des sous-régions ainsi créés. Chaque division d'une région en sous-région en fonction de la valeur de l'une des variables explicatives des individus devient de cette manière un noeud d'une arborescence de décision. Cette arborescence de décision va être utilisée pour discriminer $X$, après avoir été élaguée : en effet, un arbre complètement développé, n'ayant pas été élagué après entrainement, sera généralement relativement profond, et donc potentiellement trop complexe, et manquera de flexibilité. On cherchera donc via cette opération d'élagage, à trouver le meilleur compromis possible entre robustesse et spécificité.

\paragraph{Implémentation}
L'implémentation des arbres de décision est complexe, ainsi dans le cadre de notre TP, nous avons utilisé la librairie \verb+tree+ du langage \verb+R+ au sein d'un script (\verb+tree.R+) afin d'utiliser ce modèle de discrimination et de le tester.

\chapter{Taux d'erreurs et intervalles de confiance}
\section{Evaluation des classifieurs}
\paragraph{Méthode de l'ensemble de validation}
De la même manière que nous avions pu le faire au cours du TP précédent, nous avons utilisé la méthode de l'ensemble de validation pour estimer le risque, se ramenant dans le cas d'un cout $[0,1]$ à estimer les taux d'erreurs $\varepsilon$ de nos classifieurs.

\paragraph{Intervalles de confiance}
De manière similaire à ce qui avait été démontré au cours du TP précédent, nous estimons nos intervalles de confiance en appliquant la formule suivante. Si on note $E_j$ l'estimateur du taux d'erreur $\varepsilon$ et $u_{1-\frac{\alpha}{2}}$ un quantile de la loi normale centrée réduite pour une valeur choisie de $\alpha$, l'expression de l'intervalle de confiance correspondant s'écrit :

$$IC = [\frac{E_j}{n} - u_{1 - \frac{\alpha}{2}}\sqrt{\frac{\frac{E_j}{n} (1 - \frac{E_j}{n})}{n}} ; \frac{E_j}{n} + u_{1 - \frac{\alpha}{2}}\sqrt{\frac{\frac{E_j}{n} (1 - \frac{E_j}{n})}{n}}]$$


Dans le cadre de ce TP, nous avons choisi fixer $\alpha = 0.05$.

\chapter{Résultats expérimentaux}
\paragraph{Introduction}
Afin de comparer les performances des différents classifieurs, nous les testons sur différents jeux de données :
\begin{itemize}
    \item \textbf{Synth1, Synth2, Synth3} : trois jeux de données différents, tous des jeux de données simulées, dont les données suivent dans chaque classe une loi normale multivariée.
    \item \textbf{Pima} : Ces données correspondent à des données de diagnostique de diabète chez des individus d'une population d'amérindiens.
    \item \textbf{Breast Cancer Wisconsin - BCW} : Ces données correspondent à des données de diagnostique de gravité d'une tumeur à partir de descripteurs physiologiques.
\end{itemize}

\section{Résultats}
\subsection{Notation}
\paragraph{Note}
Au sein de cette section, nous utiliserons la notation suivante :
\begin{itemize}
    \item \textbf{ADQ, ADL, NBA} : Désignent respectivement l'Analyse Discriminante Quadratique, Linéaire et le classifieur Bayesien naïf.
    \item \textbf{LOGBT, LOGBF, LOGQT, LOGQF} : Désignent respectivement la Regression Logistique binaire classique, et la Regression logistique Quadratique (LOGB/LOGQ), avec une valeur de flag \verb+intercept+ égale à \verb+TRUE+ ou \verb+FALSE+ (ex : LOGBT/LOGBF ou LOGQT / LOGQF), c'est à dire au sein de laquelle on a ajouté une ordonnée à l'origine à notre vecteur $w$ ou non.
    \item \textbf{TREE} : Désigne les Arbres de décision.
\end{itemize}

\newpage
\subsection{Synth1}
\paragraph{Résultats}
Le tableau ci-dessous récapitule les taux d'erreurs et intervalles de confiance de nos différents classifieurs sur le jeu de données \verb+Synth1+ :

\begin{table}[h!]
    \centering
    \caption{Estimations des taux d'erreurs et intervalles de confiances, par classifieur, sur le jeu de données "Synth1"}
    \label{tab:table1}
    \def\arraystretch{1.5}
    \begin{tabular}{c||c|c|c|c}
        \hline
        & ADQ & ADL & NBA & TREE\\
        \hline
        $\varepsilon$ & 0.025 & 0.033 & 0.033
        & 0.048\\
        \hline
        $IC$ & $[0.008 ; 0.041]$ & $[0.014 ; 0.053]$ & $[0.014 ; 0.052]$
        & $[0.026 ; 0.072]$\\
        \hline
        \hline
        & LOGBT & LOGBF & LOGQT & LOGQF\\
        \hline
        $\varepsilon$ & 0.025 & 0.039 & 0.025 & 0.036\\
        \hline
        $IC$ & $[0.008 ; 0.042]$ & $[0.008 ; 0.042]$ & $[0.008 ; 0.042]$ & $[0.016 ; 0.056]$\\
        \hline
        \hline
    \end{tabular}
\end{table}

\paragraph{Allures des frontières de décision}
Suivent ci-dessous le tracé des frontières de décision obtenues avec les différents classifieurs.

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.7\textwidth]{results/adq/adq-Synth1.pdf}
    \caption{Frontières de décision de l'ADQ sur le dataset Synth1}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/adl/adl-Synth1.pdf}
    \caption{Frontières de décision de l'ADL sur le dataset Synth1}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/nba/nba-Synth1.pdf}
    \caption{Frontières de décision du NBA sur le dataset Synth1}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/binlrt/binlrt-Synth1.pdf}
    \caption{Frontières de décision de la LOGBT sur le dataset Synth1}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/binlrf/binlrf-Synth1.pdf}
    \caption{Frontières de décision de la LOGBF sur le dataset Synth1}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/quadlrt/quadlrt-Synth1.pdf}
    \caption{Frontières de décision de la LOGQT sur le dataset Synth1}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/quadlrf/quadlrf-Synth1.pdf}
    \caption{Frontières de décision de la LOGQF sur le dataset Synth1}
\end{center}
\end{figure}


\clearpage
\paragraph{Interprétation}
A ce stade, difficile pour nous d'interpreter (sans comparaison possible). Notons tout de même le fait que la plupart des classifieurs ont un taux d'erreur relativement bas. Notons également que la différence entre ADL et NBA est très faible pour ce dataset. Notons finalement que le fait de rajouter une ordonnée à l'origine dans le cas de la Regression Logistique semble permettre au classifieur de mieux épouser la forme des classes à discriminer.

\newpage
\subsection{Synth2}
\paragraph{Résultats}
Le tableau ci-dessous récapitule les taux d'erreurs et intervalles de confiance de nos différents classifieurs sur le jeu de données \verb+Synth2+ :

\begin{table}[h!]
    \centering
    \caption{Estimations des taux d'erreurs et intervalles de confiances, par classifieur, sur le jeu de données "Synth2"}
    \label{tab:table1}
    \def\arraystretch{1.5}
    \begin{tabular}{c||c|c|c|c}
        \hline
        & ADQ & ADL & NBA & TREE\\
        \hline
        $\varepsilon$ & 0.011 & 0.012 & 0.015
        & 0.036\\
        \hline
        $IC$ & $[0.000 ; 0.022]$ & $[0.000 ; 0.023]$ & $[0.002 ; 0.029]$
        & $[0.016 ; 0.056]$\\
        \hline
        \hline
        & LOGBT & LOGBF & LOGQT & LOGQF\\
        \hline
        $\varepsilon$ & 0.013 & 0.014 & 0.014 & 0.013\\
        \hline
        $IC$ & $[0.000 ; 0.025]$ & $[0.001 ; 0.026]$ & $[0.001 ; 0.026]$ & $[0.001 ; 0.026]$\\
        \hline
        \hline
    \end{tabular}
\end{table}

\paragraph{Allures des frontières de décision}
Suivent ci-dessous le tracé des frontières de décision obtenues avec les différents classifieurs.

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.7\textwidth]{results/adq/adq-Synth2.pdf}
    \caption{Frontières de décision de l'ADQ sur le dataset Synth2}
\end{center}
\end{figure}

\newpage
\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/adl/adl-Synth2.pdf}
    \caption{Frontières de décision de l'ADL sur le dataset Synth2}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/nba/nba-Synth2.pdf}
    \caption{Frontières de décision du NBA sur le dataset Synth2}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/binlrt/binlrt-Synth2.pdf}
    \caption{Frontières de décision de la LOGBT sur le dataset Synth2}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/binlrf/binlrf-Synth2.pdf}
    \caption{Frontières de décision de la LOGBF sur le dataset Synth2}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/quadlrt/quadlrt-Synth2.pdf}
    \caption{Frontières de décision de la LOGQT sur le dataset Synth2}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/quadlrf/quadlrf-Synth2.pdf}
    \caption{Frontières de décision de la LOGQF sur le dataset Synth2}
\end{center}
\end{figure}

\clearpage
\paragraph{Interprétation}
Les performances de nos classifieurs issus de l'Analyse Discriminante comme de la Regression Logistique s'améliorent sur ce dataset. Seul l'Arbre de décision ne semble pas réellement progresser. Il semblerait que l'amélioration des performances de nos classifieurs reposant sur le calcul de probabilités à posteriori d'appartenance des individus au classes soit liée au fait que la séparation entre les deux classes est plus nette, le nombre d'outliers moins élevé permet de discriminer les individus de test à l'aide d'une frontière de décision qui en est plus pertinente.

\newpage
\subsection{Synth3}
\paragraph{Résultats}
Le tableau ci-dessous récapitule les taux d'erreurs et intervalles de confiance de nos différents classifieurs sur le jeu de données \verb+Synth3+ :

\begin{table}[h!]
    \centering
    \caption{Estimations des taux d'erreurs et intervalles de confiances, par classifieur, sur le jeu de données "Synth3"}
    \label{tab:table1}
    \def\arraystretch{1.5}
    \begin{tabular}{c||c|c|c|c}
        \hline
        & ADQ & ADL & NBA & TREE\\
        \hline
        $\varepsilon$ & 0.012 & 0.023 & 0.024
        & 0.042\\
        \hline
        $IC$ & $[0.000 ; 0.023]$ & $[0.007 ; 0.040]$ & $[0.007 ; 0.040]$
        & $[0.020 ; 0.064]$\\
        \hline
        \hline
        & LOGBT & LOGBF & LOGQT & LOGQF\\
        \hline
        $\varepsilon$ & 0.018 & 0.024 & 0.013 & 0.013\\
        \hline
        $IC$ & $[0.004 ; 0.033]$ & $[0.008 ; 0.041]$ & $[0.000 ; 0.025]$ & $[0.000 ; 0.025]$\\
        \hline
        \hline
    \end{tabular}
\end{table}

\paragraph{Allures des frontières de décision}
Suivent ci-dessous le tracé des frontières de décision obtenues avec les différents classifieurs.

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.7\textwidth]{results/adq/adq-Synth3.pdf}
    \caption{Frontières de décision de l'ADQ sur le dataset Synth3}
\end{center}
\end{figure}

\newpage
\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/adl/adl-Synth3.pdf}
    \caption{Frontières de décision de l'ADL sur le dataset Synth3}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/nba/nba-Synth3.pdf}
    \caption{Frontières de décision du NBA sur le dataset Synth3}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/binlrt/binlrt-Synth3.pdf}
    \caption{Frontières de décision de la LOGBT sur le dataset Synth3}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/binlrf/binlrf-Synth3.pdf}
    \caption{Frontières de décision de la LOGBF sur le dataset Synth3}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/quadlrt/quadlrt-Synth3.pdf}
    \caption{Frontières de décision de la LOGQT sur le dataset Synth3}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
    \includegraphics[width=0.6\textwidth]{results/quadlrf/quadlrf-Synth3.pdf}
    \caption{Frontières de décision de la LOGQF sur le dataset Synth3}
\end{center}
\end{figure}

\clearpage
\paragraph{Interprétation}
Nous remarquons ici un écart de performances important entre l'ADQ d'une part, et l'ADL et le NBA d'autre part. Il semblerait qu'ici l'hypothèse d'homoscédasticité des données donne de bien moins bons résultats, ce qui nous laisse penser que le paramètre covariance utilisé pour générer chacune des classes est différent. Notons également la relative constance des résultats obtenus par les arbres de décision, en deça des performances de l'Analyse Discriminante et de la Regression Logistique, tout en restant respectable.



\end{document}
